{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import *\n",
    "from gensim import similarities\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.matutils import softcossim\n",
    "\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "googlenews = GoogleNews()\n",
    "\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import docx\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "\n",
    "from parsers import Google_parser as gp\n",
    "\n",
    "\n",
    "## запрос по api\n",
    "import requests\n",
    "import json\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "import gensim.downloader as api\n",
    "# Download the models\n",
    "#fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "#glove_model300 = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "from emailing import sendmailto\n",
    "\n",
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return words #' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста\n",
    "\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    A function that places a hyperlink within a paragraph object.\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок транспортных новостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# transport_topiclist = [ 'maritime AND transport', 'port AND freight', '+cargo', 'transport AND delay', 'container AND ship', '+bulk', 'bulk AND carrier',\n",
    "#                        '+tanker', '+TEU', 'auto AND industry', '+freight', 'refrigerated AND ship', '+reefer', '+supertanker', 'cargo AND ship']\n",
    "\n",
    "transport_topiclist = [ '+maritime +transport', '+maritime +transport', '+cargo', '+container +ship', '+bulk', '+bulk +carrier',\n",
    "                       '+tanker', '+TEU', '+auto +industry', '+freight', '+refrigerated +ship', '+reefer', '+supertanker', '+cargo +ship']\n",
    "transport_all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in transport_topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    all_articles = newsapi.get_everything(q=request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(all_articles['articles'])):\n",
    "        url  = all_articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            all_articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    transport_all_articles.append(all_articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "transport_all_contents = []\n",
    "transport_all_titles = []\n",
    "transport_all_urls = []\n",
    "for article_list in transport_all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        transport_all_contents.append(item['content'])\n",
    "        transport_all_titles.append(item['title'])\n",
    "        transport_all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(transport_all_contents)):\n",
    "    try:\n",
    "        transport_all_contents[i] = summarize(text = transport_all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in transport_all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "    \n",
    "# text_0 = mydict.doc2bow(tokenized_list[22])\n",
    "# text_1 = mydict.doc2bow(tokenized_list[32])\n",
    "     \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (transport_all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(transport_all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(transport_all_contents[text_index])\n",
    "            unique_urls.append(transport_all_urls[text_index])\n",
    "            unique_titles.append(transport_all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости транспорта'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'transport_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in mydict.token2id.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Выгружаются новости... ---\n",
      "5\n",
      "10\n",
      "--- Завершено. На выгрузку затрачено 6.489573955535889 секунд ---\n"
     ]
    }
   ],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# transport_topiclist = [ 'maritime AND transport', 'port AND freight', '+cargo', 'transport AND delay', 'container AND ship', '+bulk', 'bulk AND carrier',\n",
    "#                        '+tanker', '+TEU', 'auto AND industry', '+freight', 'refrigerated AND ship', '+reefer', '+supertanker', 'cargo AND ship']\n",
    "\n",
    "transport_topiclist = [ 'cargo freight']\n",
    "transport_all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in transport_topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    all_articles = newsapi.get_everything(q=request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=10)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(all_articles['articles'])):\n",
    "        url  = all_articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            all_articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    transport_all_articles.append(all_articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "transport_all_contents = []\n",
    "transport_all_titles = []\n",
    "transport_all_urls = []\n",
    "for article_list in transport_all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        transport_all_contents.append(item['content'])\n",
    "        transport_all_titles.append(item['title'])\n",
    "        transport_all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(transport_all_contents)):\n",
    "    try:\n",
    "        transport_all_contents[i] = summarize(text = transport_all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in transport_all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return    words#' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста\n",
    "\n",
    "tokenized_list = []\n",
    "for doc in transport_all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "model = TfidfModel(mycorpus)  # fit model\n",
    "vector = model[mycorpus[0]]\n",
    "corpus_tfidf = model[mycorpus] #apply transformation to the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transport_all_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boeing’s BA 777 freighter would gain the most unless Airbus sees the prospect of orders from blue-chip Chinese airlines as motivation to develop an A350 freighter and break Boeing’s monopoly on new long-range freight aircraft.\n",
      "Shanghai is the world’s third-largest air cargo hub but local airline China Eastern has a cargo division with only eight freighters.\n",
      "If carriers want to increase freight aircraft, new sales are likely since converting old passenger aircraft to freighters is seen as inefficient for regular long-haul use.\n",
      "MORE FROM FORBES These Airlines Will Next Remove Seats To Carry Cargo And Medical Supplies On Passenger Aircraft Turned Into Freighters\n",
      "\n",
      "\n",
      "SEATTLE, April 29, 2020 /PRNewswire/ -- Alaska Air Cargo announced it will fly passenger jets as cargo-only flights to carry essential goods like mail, medical equipment, e-commerce packages and food throughout its domestic network.\n",
      "we serve: 19 stations, only three connect by road Around 60% of our cargo business touches the state of Alaska in some way\n",
      "Alaska will be utilizing passenger jets as freighter only aircraft to maximize critical cargo shipments of essential goods.\n",
      "Alaska will be utilizing passenger jets as freighter only aircraft to maximize critical cargo shipments of essential goods.\n"
     ]
    }
   ],
   "source": [
    "print(transport_all_contents[0])\n",
    "print('\\n')\n",
    "print(transport_all_contents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-744d888f3f96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcosine_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransport_all_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'transport'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-7d2e7f35ccfd>\u001b[0m in \u001b[0;36mcosine_sim\u001b[1;34m(text1, text2)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \"\"\"\n\u001b[0;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1859\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "cosine_sim(transport_all_contents[0], ['transport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordListCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6a0a558ebc7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \"\"\"\n\u001b[0;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1859\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1121\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m         \u001b[0manalyze\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m         \u001b[0mj_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mbuild_analyzer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'word'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m             \u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             self._check_stop_words_consistency(stop_words, preprocess,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mget_stop_words\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstop\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_check_stop_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_stop_words_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_stop_list\u001b[1;34m(stop)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# assume it's a collection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'WordListCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "vect=TfidfVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "tfidf=vect.fit_transform(tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок Агро новостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "            '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', \n",
    "             '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el AND +nino', '+usa AND +drought']\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "for article_list in all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        all_contents.append(item['content'])\n",
    "        all_titles.append(item['title'])\n",
    "        all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        all_contents[i] = summarize(text = all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "          \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(all_contents[text_index])\n",
    "            unique_urls.append(all_urls[text_index])\n",
    "            unique_titles.append(all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости агросектора'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'agro_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+energy AND +belarus', '+energy AND +kazakhstan', '+energy AND +uzbekistan','+renewable AND +energy' ]\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "for article_list in all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        all_contents.append(item['content'])\n",
    "        all_titles.append(item['title'])\n",
    "        all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        all_contents[i] = summarize(text = all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "          \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(all_contents[text_index])\n",
    "            unique_urls.append(all_urls[text_index])\n",
    "            unique_titles.append(all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости агросектора'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'agro_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+energy AND +belarus', '+energy AND +kazakhstan', '+energy AND +uzbekistan','+renewable AND +energy' ]\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=5)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(mycorpus, smartirs='ntc')\n",
    "for doc in tfidf[mycorpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "    \n",
    "# text_0 = mydict.doc2bow(tokenized_list[22])\n",
    "# text_1 = mydict.doc2bow(tokenized_list[32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c']        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (transport_all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(transport_all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(transport_all_contents[text_index])\n",
    "            unique_urls.append(transport_all_urls[text_index])\n",
    "            unique_titles.append(transport_all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    A function that places a hyperlink within a paragraph object.\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости транспорта'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'transport_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model300.most_similar('transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #формируем документ из коротких саммари новостей\n",
    "    document = Document()\n",
    "    doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "    document.add_heading(doc_header)\n",
    "    document.add_heading(doc_datestamp, level = 1)\n",
    "    for i in range (0, len(all_articles['articles'])):\n",
    "        header = all_articles['articles'][i]['title']\n",
    "        document.add_heading(header, level = 1)\n",
    "        try:\n",
    "            content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "        except:\n",
    "            content = all_articles['articles'][i]['content']\n",
    "        document.add_paragraph(content)\n",
    "        url = all_articles['articles'][i]['url']\n",
    "        document.add_paragraph(url)\n",
    "\n",
    "    #doc_name = str(request_topic) + '.docx' \n",
    "    doc_name = str(request_topic) + '_summary100.docx'\n",
    "    document.save(doc_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### запрос по всей базе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "request_topic = 'china'\n",
    "\n",
    "# получение данных по теме\n",
    "all_articles = newsapi.get_everything(q=request_topic,\n",
    "                                      from_param=previous_day,\n",
    "                                      to=current_date,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=10)\n",
    "\n",
    "#Формирование служебных переменных\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "#попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text) \n",
    "        all_articles['articles'][i]['content'] = text \n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')\n",
    "\n",
    "#формируем документ из коротких саммари новостей\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    try:\n",
    "        content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    except:\n",
    "        content = all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запрос по конкретной стране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_topic = 'transportation'\n",
    "\n",
    "# получение данных по теме\n",
    "top_headlines = newsapi.get_top_headlines(q=request_topic,\n",
    "                                          #category='business',\n",
    "                                          language='en',\n",
    "                                          #country='cn')\n",
    "                                         )\n",
    "\n",
    "top_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_headlines = newsapi.get_top_headlines(q=request_topic,\n",
    "                                          #category='business',\n",
    "                                          language='en',\n",
    "                                          country='us')\n",
    "top_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "request_topic = 'china land reform'\n",
    "\n",
    "# получение данных по теме\n",
    "top_headlines = newsapi.get_top_headlines(q=request_topic,\n",
    "                                          category='business',\n",
    "                                          language='en',\n",
    "                                          pageSize = 10,\n",
    "                                          country='cn')\n",
    "\n",
    "#Формирование служебных переменных\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "#попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text) \n",
    "        all_articles['articles'][i]['content'] = text \n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')\n",
    "\n",
    "#формируем документ из коротких саммари новостей\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    try:\n",
    "        content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    except:\n",
    "        content = all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Формирование массива ссылок из выдачи Api\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "#сохраним копию выгрузки с коротким содержанием новостей\n",
    "all_articles_short = all_articles.copy()\n",
    "\n",
    "#попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    gnews_links.append(url)\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text) \n",
    "        all_articles['articles'][i]['content'] = text \n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "document = Document()\n",
    "document.add_heading(request_topic)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    #all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    #all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок выгрузки из ГУГЛа и предобработки новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_topiclist_rus = ['Avtovaz', 'Lada', 'Kia', 'Hyundai ', 'Avtodizel', 'GAZ', 'Gazel', 'KAMAZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция предобработки русскоязычных текстов\n",
    "from nltk.corpus import stopwords\n",
    "stop_rus = stopwords.words('russian')\n",
    "\n",
    "def prepare_text_russian(text, stop = stop_rus, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return    words#' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return    words#' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def googleparser (topic, depth):\n",
    "    '''\n",
    "    topic - тема новостного запроса\n",
    "    depth - число опрашиваемых страниц поисковой выдачи\n",
    "    \n",
    "    Возвращает два списка: с текстами новостей и с проблемными ссылками \n",
    "    \n",
    "    '''\n",
    "    # 1. формирование массива ссылок\n",
    "    gnews_links = []\n",
    "    gnews=[]\n",
    "    googlenews.search(topic)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('--- Формируется массив ссылок... ---')\n",
    "\n",
    "    for i in range(1,depth):\n",
    "        googlenews.clear()\n",
    "        googlenews.getpage(i)\n",
    "        for j in range(0, len(googlenews.gettext())):\n",
    "            gnews.append(googlenews.gettext()[j])\n",
    "            gnews_links.append(googlenews.getlinks()[j])\n",
    "\n",
    "    print(\"--- На формирование массива затрачено %s секунд ---\" % (time.time() - start_time))             \n",
    "    print('--- Завершено. Получено %s ссылок ---' % len(gnews_links))   \n",
    "    \n",
    "    # 2. выгрузка новостей и формирование массива текстов\n",
    "    \n",
    "    body = []\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    for url in gnews_links:\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body.append(text)\n",
    "        except:\n",
    "            error_link.append(gnews_links[count]) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "\n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))             \n",
    "    return body, error_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews = GoogleNews('ru')\n",
    "topic = transport_topiclist_rus[0]\n",
    "googlenews.search(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем запрашивать новости из гугла на русском языке транслитом (т.к. модуль почему-то не работает с выгрузками на англ)\n",
    "googlenews = GoogleNews('ru')\n",
    "all_articles = []\n",
    "for topic in transport_topiclist_rus:\n",
    "    googlenews.clear()\n",
    "    googlenews.search(topic)\n",
    "    for item in googlenews.result():\n",
    "        all_articles.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем массивы с данными о заглавиях, ссылках и содержании новостей\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "\n",
    "for article_number in range(0,len(all_articles)):\n",
    "    url  = all_articles[article_number]['link']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html, language = 'ru')\n",
    "    except:\n",
    "        text = all_articles[article_number]['desc']\n",
    "    if text != '':\n",
    "        all_contents.append(text)\n",
    "        all_titles.append(all_articles[article_number]['title'])\n",
    "        all_urls.append(url)   \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_text(all_contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        summary = summarize(text = all_contents[i], word_count = 100)\n",
    "        if len(summary) > 0:\n",
    "            all_contents[i] = summary\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec, WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "from gensim.models.keyedvectors import WordEmbeddingSimilarityIndex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем матрицу схожести на основе модели Word2vec\n",
    "#здесь считается косинусная близость для всех пар векторов (слов)\n",
    "termsim_index = WordEmbeddingSimilarityIndex(word2vec_model300)\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем косинусную близость между всеми документами\n",
    "docsim_index = SoftCosineSimilarity(bag_of_words, similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест выгрузок по одной теме за раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_topic_list = ['maritime transport', 'port freight', 'container ship', 'bulk carrier', 'tanker', 'TEU', 'auto industry', 'refrigerated ship', 'reefer', 'supertanker', 'cargo ship']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c', 'nintendo', 'docker', 'desktop', 'spacex', 'windows']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agro_topic_list = ['+china AND (+red +line)', '+china +((agriculture | food) AND subsidy)', '+china +gmo', '+china +(food AND reserve)', '+china +(agriculture AND policy)', '+india AND +fertilizer', '+monsoon | +(el AND nino) | (+USA  +drought)', 'brazil AND (inflation | real)', \n",
    "#              '+argentina +(export AND tariff)',  '+EU | +Europe +(food AND quota)']\n",
    "agro_topic_list = ['china +red +line', 'china agriculture food subsidy', 'china +gmo', 'china food reserve', 'china agriculture policy', 'india fertilizer', 'monsoon el nino drought', 'brazil inflation real', \n",
    "              'argentina export tariff',  'EU food quota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "itt_topic_list = ['semiconductors market', '+software +market +volume', 'telecommunications market volume', 'iaas | paas | saas', '+it +market', 'computer sales', 'IT coronavirus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_news_list_google(topiclist, topicname):\n",
    "    articles_base = {}\n",
    "    for topic in topiclist:\n",
    "        # выгрузка статей по одному запросу\n",
    "        all_articles = []\n",
    "        googlenews.clear()\n",
    "        googlenews.search(topic)\n",
    "        for item in googlenews.result()[0:5]:\n",
    "            all_articles.append(item)\n",
    "            \n",
    "        #dict of topics and data for each topic\n",
    "\n",
    "        articles_base[topic] = all_articles\n",
    "        \n",
    "    # формируем границы периода времени, за которое будем запрашивать новости\n",
    "    current_date = datetime.today()\n",
    "    previous_day = current_date - timedelta(days = 1)\n",
    "    current_date = current_date.strftime('%Y-%m-%d')\n",
    "    previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "    #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "    document = Document()\n",
    "\n",
    "    doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    document.add_heading(doc_header)\n",
    "    \n",
    "    for topic in articles_base:\n",
    "        count = 1\n",
    "        header = 'Новости по запросу: ' + str(topic)\n",
    "        document.add_heading(header, level = 2)\n",
    "        for item in articles_base[topic]:\n",
    "            tokens = prepare_text(item['title'])\n",
    "            raise_flag_excludeword = 0\n",
    "            for token in tokens:\n",
    "                if token in exclude_list:\n",
    "                    raise_flag_excludeword = 1\n",
    "            if raise_flag_excludeword == 0:         \n",
    "                url = item['link']\n",
    "                p = document.add_paragraph()\n",
    "                link_text = str(count) + '. ' + item['title'] \n",
    "                url = add_hyperlink(p, url, link_text)\n",
    "                count += 1\n",
    "\n",
    "    #doc_name = str(request_topic) + '.docx' \n",
    "    doc_name = str(topicname) + '_' + str(current_date) + '.docx'\n",
    "    document.save(doc_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_news_list_newsapi(topiclist, topicname):\n",
    "    articles_base = {}\n",
    "    for topic in topiclist:\n",
    "\n",
    "        \n",
    "        # формируем границы периода времени, за которое будем запрашивать новости\n",
    "        current_date = datetime.today()\n",
    "        previous_day = current_date - timedelta(days = 1)\n",
    "        current_date = current_date.strftime('%Y-%m-%d')\n",
    "        previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "        # получение данных по теме\n",
    "        all_articles = newsapi.get_everything(q=topic,\n",
    "                                              from_param=previous_day,\n",
    "                                              to=current_date,\n",
    "                                              language='en',\n",
    "                                              sort_by='relevancy',\n",
    "                                              page_size=10)\n",
    "\n",
    "\n",
    "        articles_base[topic] = all_articles['articles']\n",
    "\n",
    "    #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "    document = Document()\n",
    "\n",
    "    doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    document.add_heading(doc_header)\n",
    "\n",
    "    for topic in articles_base:\n",
    "        count = 1\n",
    "        header = 'Новости по запросу: ' + str(topic)\n",
    "        document.add_heading(header, level = 2)\n",
    "        for item in articles_base[topic]:\n",
    "            tokens = prepare_text(item['title'])\n",
    "            raise_flag_excludeword = 0\n",
    "            for token in tokens:\n",
    "                if token in exclude_list:\n",
    "                    raise_flag_excludeword = 1\n",
    "            if raise_flag_excludeword == 0:         \n",
    "                url = item['url']\n",
    "                p = document.add_paragraph()\n",
    "                link_text = str(count) + '. ' + item['title'] \n",
    "                url = add_hyperlink(p, url, link_text)\n",
    "                count += 1\n",
    "\n",
    "    #doc_name = str(request_topic) + '.docx' \n",
    "    doc_name = str(topicname) + '_' + str(current_date) + '.docx'\n",
    "    document.save(doc_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_news_list_aggregate(topiclist, topicname):\n",
    "    # формируем границы периода времени, за которое будем запрашивать новости\n",
    "    current_date = datetime.today()\n",
    "    previous_day = current_date - timedelta(days = 1)\n",
    "    current_date = current_date.strftime('%Y-%m-%d')\n",
    "    previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "    articles_base_google = {}\n",
    "    articles_base_newsapi = {}\n",
    "    articles_base = {}\n",
    "    for topic in topiclist:\n",
    "        # выгрузка статей по одному запросу\n",
    "        all_articles = []\n",
    "        googlenews.clear()\n",
    "        googlenews.search(topic)\n",
    "        for item in googlenews.result()[0:5]:\n",
    "            all_articles.append(item)\n",
    "\n",
    "        articles_base_google[topic] = all_articles\n",
    "\n",
    "\n",
    "\n",
    "        # получение данных по теме\n",
    "        all_articles = newsapi.get_everything(q=topic,\n",
    "                                              from_param=previous_day,\n",
    "                                              to=current_date,\n",
    "                                              language='en',\n",
    "                                              sort_by='relevancy',\n",
    "                                              page_size=5)\n",
    "\n",
    "\n",
    "        articles_base_newsapi[topic] = all_articles['articles']\n",
    "\n",
    "        articles_base[topic] = articles_base_google[topic] + articles_base_newsapi[topic]\n",
    "\n",
    "    #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "    document = Document()\n",
    "\n",
    "    doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    document.add_heading(doc_header)\n",
    "\n",
    "    for topic in articles_base:\n",
    "        count = 1\n",
    "        header = 'Новости по запросу: ' + str(topic)\n",
    "        document.add_heading(header, level = 2)\n",
    "        for item in articles_base[topic]:\n",
    "            tokens = prepare_text(item['title'])\n",
    "            raise_flag_excludeword = 0\n",
    "            for token in tokens:\n",
    "                if token in exclude_list:\n",
    "                    raise_flag_excludeword = 1\n",
    "            if raise_flag_excludeword == 0: \n",
    "                try:\n",
    "                    url = item['url']\n",
    "                except:\n",
    "                    url = item['link']\n",
    "                p = document.add_paragraph()\n",
    "                link_text = str(count) + '. ' + item['title'] \n",
    "                url = add_hyperlink(p, url, link_text)\n",
    "                count += 1\n",
    "\n",
    "    doc_name = 'aggregate_' + str(topicname) + '_' + str(current_date) + '.docx'\n",
    "    document.save(doc_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_newsapi(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_google(itt_topic_list, 'ITT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_google(topic_list, 'Transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_aggregate(agro_topic_list, 'Agriculture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_aggregate(topic_list, 'Transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_aggregate(itt_topic_list, 'ITT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topiclist = ['maritime transport', 'port freight', 'container ship', 'bulk carrier', 'tanker', 'TEU', 'auto industry', 'refrigerated ship', 'reefer', 'supertanker', 'cargo ship']\n",
    "topicname = 'Trans'\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "articles_base_google = {}\n",
    "articles_base_newsapi = {}\n",
    "articles_base = {}\n",
    "for topic in topiclist:\n",
    "    # выгрузка статей по одному запросу\n",
    "    all_articles = []\n",
    "    googlenews.clear()\n",
    "    googlenews.search(topic)\n",
    "    for item in googlenews.result()[0:5]:\n",
    "        all_articles.append(item)\n",
    "\n",
    "    articles_base_google[topic] = all_articles\n",
    "\n",
    "\n",
    "\n",
    "    # получение данных по теме\n",
    "    all_articles = newsapi.get_everything(q=topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=5)\n",
    "\n",
    "\n",
    "    articles_base_newsapi[topic] = all_articles['articles']\n",
    "\n",
    "    articles_base[topic] = articles_base_google[topic] + articles_base_newsapi[topic]\n",
    "\n",
    "# #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "# document = Document()\n",
    "\n",
    "# doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "# document.add_heading(doc_header)\n",
    "\n",
    "# for topic in articles_base:\n",
    "#     count = 1\n",
    "#     header = 'Новости по запросу: ' + str(topic)\n",
    "#     document.add_heading(header, level = 2)\n",
    "#     for item in articles_base[topic]:\n",
    "#         tokens = prepare_text(item['title'])\n",
    "#         raise_flag_excludeword = 0\n",
    "#         for token in tokens:\n",
    "#             if token in exclude_list:\n",
    "#                 raise_flag_excludeword = 1\n",
    "#         if raise_flag_excludeword == 0: \n",
    "#             try:\n",
    "#                 url = item['url']\n",
    "#             except:\n",
    "#                 url = item['link']\n",
    "#             p = document.add_paragraph()\n",
    "#             link_text = str(count) + '. ' + item['title'] \n",
    "#             url = add_hyperlink(p, url, link_text)\n",
    "#             count += 1\n",
    "\n",
    "# doc_name = 'aggregate_' + str(topicname) + '_' + str(current_date) + '.docx'\n",
    "# document.save(doc_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame.from_records(articles_base['maritime transport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>media</th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>img</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maritime transport, logistics target N78 trill...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>REPORTS from global port and marine operations...</td>\n",
       "      <td>https://www.vanguardngr.com/2020/05/maritime-t...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maritime Transport Consulting Service Market 2...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Maritime Transport Consulting Service Market 2...</td>\n",
       "      <td>https://coleofduty.com/military-news/2020/05/0...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eckerö Line and DFDS started cooperation</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DFDS is one of Europe's largest maritime trans...</td>\n",
       "      <td>https://en.portnews.ru/news/295432/</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Body found on boat in Tauranga Harbour: Man ch...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The court charging document revealed the mansl...</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/news/article.cfm...</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Proposal To Trial Suburban Commuter Passenger ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>A Proposal from the Rail and Maritime Transpor...</td>\n",
       "      <td>https://www.scoop.co.nz/stories/PO2005/S00079/...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title media date  \\\n",
       "0  Maritime transport, logistics target N78 trill...              \n",
       "1  Maritime Transport Consulting Service Market 2...              \n",
       "2           Eckerö Line and DFDS started cooperation              \n",
       "3  Body found on boat in Tauranga Harbour: Man ch...              \n",
       "4  Proposal To Trial Suburban Commuter Passenger ...              \n",
       "\n",
       "                                                desc  \\\n",
       "0  REPORTS from global port and marine operations...   \n",
       "1  Maritime Transport Consulting Service Market 2...   \n",
       "2  DFDS is one of Europe's largest maritime trans...   \n",
       "3  The court charging document revealed the mansl...   \n",
       "4  A Proposal from the Rail and Maritime Transpor...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.vanguardngr.com/2020/05/maritime-t...   \n",
       "1  https://coleofduty.com/military-news/2020/05/0...   \n",
       "2                https://en.portnews.ru/news/295432/   \n",
       "3  https://www.nzherald.co.nz/nz/news/article.cfm...   \n",
       "4  https://www.scoop.co.nz/stories/PO2005/S00079/...   \n",
       "\n",
       "                                                 img source author  \\\n",
       "0  https://encrypted-tbn0.gstatic.com/images?q=tb...    NaN    NaN   \n",
       "1  https://encrypted-tbn0.gstatic.com/images?q=tb...    NaN    NaN   \n",
       "2                                                       NaN    NaN   \n",
       "3  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...    NaN    NaN   \n",
       "4                                                       NaN    NaN   \n",
       "\n",
       "  description  url urlToImage publishedAt content  \n",
       "0         NaN  NaN        NaN         NaN     NaN  \n",
       "1         NaN  NaN        NaN         NaN     NaN  \n",
       "2         NaN  NaN        NaN         NaN     NaN  \n",
       "3         NaN  NaN        NaN         NaN     NaN  \n",
       "4         NaN  NaN        NaN         NaN     NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Maritime transport, logistics target N78 trillion revenue by 2023',\n",
       "  'media': '',\n",
       "  'date': '',\n",
       "  'desc': 'REPORTS from global port and marine operations, indicate that global maritime freight transportation revenue is expected to grow from $166 billion last year to\\xa0...',\n",
       "  'link': 'https://www.vanguardngr.com/2020/05/maritime-transport-logistics-target-n78-trillion-revenue-by-2023/',\n",
       "  'img': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQXf-_FR_Yhl6juYrdIOC-_iUrt_pjeixbD12Z0ICfWlDY52kkR_wm60TbPNWI3ezeNDnkU4CyF&s'},\n",
       " {'title': 'Maritime Transport Consulting Service Market 2020 | Growth ...',\n",
       "  'media': '',\n",
       "  'date': '',\n",
       "  'desc': 'Maritime Transport Consulting Service Market 2020 | Growth Drivers, Challenges, Trends, Market Dynamics and Forecast to 2026. Maritime Transport\\xa0...',\n",
       "  'link': 'https://coleofduty.com/military-news/2020/05/06/maritime-transport-consulting-service-market-2020-growth-drivers-challenges-trends-market-dynamics-and-forecast-to-2026/',\n",
       "  'img': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRa_F4F-3meR7kS-1QuN-KhT70dZxZ0AGkR4MCzKelDqppyH-XimkvwyKxSmFzqrHy4tmda6ug&s'},\n",
       " {'title': 'Eckerö Line and DFDS started cooperation',\n",
       "  'media': '',\n",
       "  'date': '',\n",
       "  'desc': \"DFDS is one of Europe's largest maritime transport and logistics companies that operates 56 vessels on 21 routes in the Baltic, North and Mediterranean Seas\\xa0...\",\n",
       "  'link': 'https://en.portnews.ru/news/295432/',\n",
       "  'img': ''},\n",
       " {'title': 'Body found on boat in Tauranga Harbour: Man charged with ...',\n",
       "  'media': '',\n",
       "  'date': '',\n",
       "  'desc': \"The court charging document revealed the manslaughter charge related to the defendant's alleged failure to adhere to the provisions of the Maritime Transport\\xa0...\",\n",
       "  'link': 'https://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&objectid=12329962',\n",
       "  'img': 'data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=='},\n",
       " {'title': 'Proposal To Trial Suburban Commuter Passenger Rail ...',\n",
       "  'media': '',\n",
       "  'date': '',\n",
       "  'desc': 'A Proposal from the Rail and Maritime Transport Union regarding the future of Dunedin Railways Limited, which will be presented to the Dunedin City Council\\xa0...',\n",
       "  'link': 'https://www.scoop.co.nz/stories/PO2005/S00079/proposal-to-trial-suburban-commuter-passenger-rail-services-in-the-greater-dunedin-area.htm',\n",
       "  'img': ''},\n",
       " {'source': {'id': None, 'name': 'Sciencedaily.com'},\n",
       "  'author': None,\n",
       "  'title': \"Surf and turf: Green new deal should be a 'teal new deal'\",\n",
       "  'description': 'Incorporating the oceans into climate policy is essential, scientists say in a new article.',\n",
       "  'url': 'https://www.sciencedaily.com/releases/2020/05/200505094957.htm',\n",
       "  'urlToImage': None,\n",
       "  'publishedAt': '2020-05-05T13:49:57Z',\n",
       "  'content': 'Debates around the Green New Deal have largely centered around climate change concerns on land. But a group of scientists are calling on policymakers to include oceans in the deal.The Green New Deal is a legislative proposal to tackle climate change and boost… [+5424 chars]'},\n",
       " {'source': {'id': None, 'name': 'Vanguardngr.com'},\n",
       "  'author': 'Urowayino Jeremiah',\n",
       "  'title': 'Maritime transport, logistics target N78 trillion revenue by 2023',\n",
       "  'description': 'REPORTS from global port and marine operations, indicate that global maritime freight transportation revenue is expected to grow from $166 billion last year to over $205 billion in 2023. Also spending in transportation and logistics on the Internet-of-Things,…',\n",
       "  'url': 'https://www.vanguardngr.com/2020/05/maritime-transport-logistics-target-n78-trillion-revenue-by-2023/',\n",
       "  'urlToImage': 'https://i1.wp.com/www.vanguardngr.com/wp-content/uploads/2019/02/International-Maritime-Organisation.jpeg?fit=1500%2C938&ssl=1',\n",
       "  'publishedAt': '2020-05-06T08:55:31Z',\n",
       "  'content': 'As CILT calls for deliberate action to locate cargo sources\\r\\nBy Godwin Oritse, with agency report\\r\\nREPORTS from global port and marine operations, indicate that global maritime freight transportation revenue is expected to grow from $166 billion last year to … [+2859 chars]'},\n",
       " {'source': {'id': None, 'name': 'Logisticsmgmt.com'},\n",
       "  'author': 'Logistics Management',\n",
       "  'title': 'Freight Forwarding: Will high-tech upstarts make a difference this year? - Logistics Management',\n",
       "  'description': 'A wave of digital disruption surfaced in the freight forwarding sector a few years ago, with startups, suppliers and even shippers using new technologies to develop a variety of innovative trade solutions. But the arrival of the COVID-19 pandemic poses the ul…',\n",
       "  'url': 'https://www.logisticsmgmt.com/article/freight_forwarding_will_high_tech_upstarts_make_a_difference_this_year',\n",
       "  'urlToImage': 'https://www.logisticsmgmt.com/images/2020_article/LM2005_F_GL_Tech_Freight_Graph_800px.jpg',\n",
       "  'publishedAt': '2020-05-05T19:29:35Z',\n",
       "  'content': 'The art of predictive analytics and forecasting was making major strides in the global freight forwarding arena, helping all stakeholders refine real-time capacity tracking and logistics management with the major transportation modes. However, analysts for Lo… [+7655 chars]'},\n",
       " {'source': {'id': None, 'name': 'Vanguardngr.com'},\n",
       "  'author': 'Urowayino Jeremiah',\n",
       "  'title': 'Soldiers turn hire drivers at Tin can port',\n",
       "  'description': 'DESPITE complaints by the management of Tin-can Island Port over uniformed men taking up driving duties in exchange for money, the military personnel, mostly army officers have continued unhindered.\\nThe post Soldiers turn hire drivers at Tin can port appeared…',\n",
       "  'url': 'https://www.vanguardngr.com/2020/05/soldiers-turn-hire-drivers-at-tin-can-port/',\n",
       "  'urlToImage': 'https://i0.wp.com/www.vanguardngr.com/wp-content/uploads/2019/07/Apapa-Oshodi-Expressway.jpg?fit=1500%2C878&ssl=1',\n",
       "  'publishedAt': '2020-05-06T01:16:33Z',\n",
       "  'content': 'disrupts transfer of containers to off dock terminals\\r\\nBy Godfrey Bivbere\\r\\nA choked-up section of the Apapa-Oshodi Expressway, taken over by trucks. More photos on Page 37. Photos by Joe Akintola, Photo Editor, Kehinde Gbadamosi, Bunmi Azeez, and Akeem Salau.… [+2115 chars]'},\n",
       " {'source': {'id': None, 'name': 'Eurekalert.org'},\n",
       "  'author': None,\n",
       "  'title': \"Surf and turf: Green new deal should be a 'teal new deal'\",\n",
       "  'description': 'Incorporating the oceans into climate policy is essential, scientists say in a new paper.',\n",
       "  'url': 'https://www.eurekalert.org/pub_releases/2020-05/sdsu-sat050420.php',\n",
       "  'urlToImage': 'https://www.eurekalert.org/multimedia/pub/web/230973_web.jpg',\n",
       "  'publishedAt': '2020-05-05T04:00:00Z',\n",
       "  'content': 'IMAGE:\\xa0Terrestrial and ocean-based climate solutions go hand-in-hand.\\r\\n view more\\xa0\\r\\nCredit: Authors of the paper\\r\\nDebates around the Green New Deal have largely centered around climate change concerns on land. But a group of scientists are calling on policyma… [+5760 chars]'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_base['maritime transport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-134-f1f738685f11>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-134-f1f738685f11>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    )#сформируем массивы с данными о заглавиях, ссылках и содержании новостей\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#сформируем массивы с данными о заглавиях, ссылках и содержании новостей\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "\n",
    "\n",
    "for article_number in range(0,len(all_articles)):\n",
    "    url  = all_articles[article_number]['link']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html, language = 'ru')\n",
    "    except:\n",
    "        text = all_articles[article_number]['desc']\n",
    "    if text != '':\n",
    "        all_contents.append(text)\n",
    "        all_titles.append(all_articles[article_number]['title'])\n",
    "        all_urls.append(url)   \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    query = tokenized_list[text_index]\n",
    "    if query != []:\n",
    "        similarity_flag = 0\n",
    "        sims = docsim_index[mydict.doc2bow(query)]\n",
    "        for comparison_index in range(text_index + 1, len(bag_of_words)):\n",
    "            similarity_ratio = sims[comparison_index]\n",
    "            if similarity_ratio > 0.2:\n",
    "                similarity_flag = 1\n",
    "\n",
    "        if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "            #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "            #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "            exclude_count = 0\n",
    "            tokens = prepare_text(all_contents[text_index])\n",
    "            for word in tokens:\n",
    "                if word in exclude_list:\n",
    "                    exclude_count += 1\n",
    "            if exclude_count == 0:\n",
    "                unique_articles.append(all_contents[text_index])\n",
    "                unique_urls.append(all_urls[text_index])\n",
    "                unique_titles.append(all_titles[text_index])\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "\n",
    "doc_header = 'Новости транспорта' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "document.add_heading(doc_header)\n",
    "\n",
    "for topic in articles_base:\n",
    "    count = 1\n",
    "    header = 'Новости по запросу: ' + str(topic)\n",
    "    document.add_heading(header, level = 2)\n",
    "    for item in articles_base[topic]:\n",
    "        tokens = prepare_text(item['title'])\n",
    "        raise_flag_excludeword = 0\n",
    "        for token in tokens:\n",
    "            if token in exclude_list:\n",
    "                raise_flag_excludeword = 1\n",
    "        if raise_flag_excludeword == 0:         \n",
    "            url = item['url']\n",
    "            p = document.add_paragraph()\n",
    "            link_text = str(count) + '. ' + item['title'] \n",
    "            url = add_hyperlink(p, url, link_text)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'experimental_transport_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body, errors = gp.googleparser('China economy',20)\n",
    "body, errors = googleparser('Agriculture',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unlabeled_body_20200207.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    i=0\n",
    "    for item in body:\n",
    "        f.write('Story %s\\n' %(i))\n",
    "        f.write(item.replace('\\n\\n','\\n'))\n",
    "        f.write('\\n\\n')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### запрос по api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## запрос по api\n",
    "import requests\n",
    "import json\n",
    "from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ('https://newsapi.org/v2/everything?'\n",
    "      'q=maritime transport&'\n",
    "      'from=2020-03-27&'\n",
    "      'sortBy=popularity&'\n",
    "      'apiKey=8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение выгрузки новостей в базу\n",
    "json_data_agro = json.loads(response.text)\n",
    "json_data_agro['articles']\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'agro' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data_agro, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение выгрузки новостей в базу\n",
    "json_data_china = json.loads(response.text)\n",
    "json_data_china['articles']\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'china' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data_china, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###выгрузка корпуса новостей, их сохранение и предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение данных по Китаю\n",
    "all_articles = newsapi.get_everything(q='maritime transport',\n",
    "                                      from_param=previous_day,\n",
    "                                      to=current_date,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=10)\n",
    "\n",
    "#Формирование массива ссылок из выдачи Api\n",
    "news_links = []\n",
    "body_fulltext = []\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "gnews_links = []\n",
    "gnews=[]\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    gnews_links.append(url)\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text)\n",
    "        all_articles['articles'][i]['content'] = text\n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'maritime' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение данных по миру\n",
    "all_articles = newsapi.get_everything(q='world economy',\n",
    "                                      from_param=current_date,\n",
    "                                      to=current_date,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=100)\n",
    "\n",
    "#Формирование массива ссылок из выдачи Api\n",
    "news_links = []\n",
    "body_fulltext = []\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "gnews_links = []\n",
    "gnews=[]\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    gnews_links.append(url)\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text)\n",
    "        all_articles['articles'][i]['content'] = text\n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение агро новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'world' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение китайских новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'china' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(body_fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for article in json_data_china['articles']:\n",
    "    print('article', i)\n",
    "    print(article['description'])\n",
    "    i += 1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "body = []\n",
    "for article in json_data_ncov['articles']:\n",
    "    try:\n",
    "        print('article', i)\n",
    "        print(article['content'][0:258])\n",
    "        body.append(article['content'][0:258])\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    print('\\n')\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in json_data_ncov['articles']:\n",
    "    try:\n",
    "        print('article', i)\n",
    "        print(article['content'][0:258])\n",
    "        body.append(article['content'][0:258])\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in companies:\n",
    "    try:\n",
    "        body, errors = gp.googleparser(company,30)\n",
    "        with open(company + '.txt', 'w',encoding=\"utf-8\") as f:\n",
    "            i=0\n",
    "            for item in body:\n",
    "                f.write('Story %s\\n' %(i))\n",
    "                f.write(item.replace('\\n\\n','\\n'))\n",
    "                f.write('\\n\\n')\n",
    "                i += 1\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in body_fulltext:\n",
    "    prepared_texts.append(prepare_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Блок выгрузки линков и заголовков новостей из гугла\n",
    "#новости выгружаются по заданной теме, глубина выгрузки - 10 страниц\n",
    "\n",
    "topic = 'China'\n",
    "gnews_links = []\n",
    "gnews=[]\n",
    "googlenews.search(topic)\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "print('--- Формируется массив ссылок... ---')\n",
    "\n",
    "for i in range(1,2):\n",
    "    googlenews.clear()\n",
    "    googlenews.getpage(i)\n",
    "    for j in range(0, len(googlenews.gettext())):\n",
    "        gnews.append(googlenews.gettext()[j])\n",
    "        gnews_links.append(googlenews.getlinks()[j])\n",
    "\n",
    "print(\"--- На формирование массива затрачено %s секунд ---\" % (time.time() - start_time))             \n",
    "print('--- Заврешено. Выгружено %s статей ---' % len(gnews_links))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#блок выгрузки новостей и формирования массива текстов\n",
    "body = []\n",
    "count = 0\n",
    "error_position = [] #массив с номерами битых ссылок\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "for url in gnews_links:\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body.append(text)\n",
    "    except:\n",
    "        error_position.append(count) #иногда попадаются битые ссылки. Здесь мы будем сохранять их позиции\n",
    "        pass\n",
    "    count += 1\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Inputs of LDA model: Dictionary and Corpus \n",
    "#(doc2bow array of id's of words and their frequency)\n",
    "\n",
    "dct = corpora.Dictionary(prepared_texts)\n",
    "corpus = [dct.doc2bow(item) for item in prepared_texts]\n",
    "#dct.save('news.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('news.model',corpus)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and update the model on new data\n",
    "\n",
    "lda_model = LdaMulticore.load(datapath(\"C:/Users/Egran/YandexDisk/Work/news_project/lda_model\"))\n",
    "lda_model.update(corpus)\n",
    "lda_model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train new LDA model\n",
    "\n",
    "lda_daily_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=9,\n",
    "                         passes=50,\n",
    "                         chunksize=100,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "#lda__daily_model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the topics\n",
    "#lda_model.print_topics(-1)\n",
    "lda_daily_model.show_topics(num_topics=9, num_words=10, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_daily_model.top_topics(texts=prepared_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_daily_model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_daily_model, texts=prepared_texts, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация выводов тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод тем по обученной LDA модели\n",
    "# 1. Wordcloud of Top N words in each topic\n",
    "\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=5,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_daily_model.show_topics(formatted=False)\n",
    "#topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf на векторах\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# создание случайной выборки\n",
    "sampling_tfidf = random.choices(corpus_tfidf, k=30)\n",
    "\n",
    "index = similarities.MatrixSimilarity(sampling_tfidf)\n",
    "sims = index[sampling_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.heatmap(data=sims, cmap = 'Spectral').set(xticklabels=[], yticklabels=[])\n",
    "plt.title(\"Матрица близости\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more on visuals\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dct.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерактивная визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_daily_model, corpus, dct, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста для рейтера (он аналогичен блоку выше, но возвращает строки, а не отдельные слова)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['china', 'chinese','china\\'s','china’s'] #в стоп список включаем слова, которые являются перекрестными в разных темах\n",
    "lemmatizator = WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def reuter_prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in words if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return ' '.join(words)#words  #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def china_summary_reuters(depth):\n",
    "    \"\"\"\n",
    "    Подготовка резюме по основным новостям по Китаю\n",
    "    depth - количество заголовков для просмотра на странице с последними новостями по Китаю\n",
    "    \"\"\"\n",
    "      \n",
    "    #Выгружаем все свежие заголовки про Китай\n",
    "    r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "    #r1 = requests.get('https://www.reuters.com/search/news?blob=metals')\n",
    "    coverpage =  r1.content\n",
    "    soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "    coverpage_news_r = soup.find_all('h2', class_='FeedItemHeadline_headline')\n",
    "\n",
    "    #Опционально можно вывести отдельно все заголовки:\n",
    "#     for i in range(0, len(coverpage_news_r)):\n",
    "#         print(coverpage_news_r[i].get_text())\n",
    "#         print('///')\n",
    "\n",
    "    # Опционально можно парсить все статьи по существующим заголовкам без ограничения глубины\n",
    "    #number_of_articles = len(coverpage_news_r)\n",
    "    \n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('---Формируется подборка статей---')\n",
    "\n",
    "    #формируем массив статей\n",
    "    for n in range(0, depth):\n",
    "\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news_r[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news_r[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "        try:\n",
    "            x = body[0].find_all('p')\n",
    "        except:\n",
    "            pass\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "    print(\"--- Подборка сформирована. Затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    \n",
    "    #Проходим по массиву выгруженных новостей и выполняем предобработку\n",
    "    print('---Предобработка---')\n",
    "    prepared_texts = []\n",
    "    for item in news_contents:\n",
    "        prepared_texts.append(reuter_prepare_text(item))\n",
    "    print('---Предобработка выполнена---')\n",
    "    print('///')\n",
    "    #задаем ключевые слова по темам\n",
    "    topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR','trade']\n",
    "\n",
    "    # отбор новостей по ключевым словам и суммаризация\n",
    "    i = 0\n",
    "    for item in prepared_texts:\n",
    "        counter = 0\n",
    "        for word in item.split(' '):\n",
    "            if word in topics_econ:\n",
    "                counter += 1\n",
    "        if counter > 2:\n",
    "            print('Economic story %s' % i)\n",
    "            print(summarize(item, word_count=60))\n",
    "            print('Ссылка на статью:', list_links[i])\n",
    "            print('///')\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Старые эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выгружаем все свежие заголовки про Китай\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "#r1 = requests.get('https://www.reuters.com/search/news?blob=metals')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('h2', class_='FeedItemHeadline_headline')\n",
    "\n",
    "#Выводим заголовки\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(coverpage_news_r[i].get_text())\n",
    "    print('///')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#задаем ключевые слова по темам\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR','trade']\n",
    "topics_metals = ['metal', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "print(summarize(prepared_texts[0], word_count=60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбор новостей по ключевым словам\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_econ:\n",
    "            counter += 1\n",
    "    if counter > 2:\n",
    "        print('Economic story %s', i)\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок металлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выгружаем все свежие заголовки про металлы из рейтерс\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "#r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "#https://www.reuters.com/news/archive/metals-news\n",
    "r1 = requests.get('https://www.reuters.com/search/news?blob=Metals&sortBy=date&dateRange=all')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('h3', class_='search-result-title')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(coverpage_news_r[i].get_text())\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = 'https://www.reuters.com'+coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 3 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Парсинг статей про металлы из архива\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('https://www.reuters.com/news/archive/metals-news')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('div', class_='story-content')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(re.sub('\\t','',coverpage_news_r[i].get_text()))\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = 'https://www.reuters.com'+coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 1 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Парсинг статей про металлы с сайта Kitco\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('http://www.kitcometals.com/news/')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('td', class_='text')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(re.sub('\\t','',coverpage_news_r[i].get_text()))    #таким образом убираем тупые пробелы перед заглавием\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('article', itemprop='articleBody')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 1 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
